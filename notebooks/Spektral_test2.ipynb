{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af6db94-c3eb-466c-9b40-7d6e43d97e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QM9 dataset.\n",
      "Reading SDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1329.76it/s]\n",
      "/Users/isidre/Documents/UAB/TFM/FootballAnalyticsTFM/.venv1/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: The adjacency matrix of dtype <dtype: 'int64'> is incompatible with the dtype of the node features <dtype: 'float32'> and has been automatically cast to <dtype: 'float32'>.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 22116324.0\n",
      "Loss: 22056938.0\n",
      "Loss: 21929188.0\n",
      "Loss: 21826488.0\n",
      "Loss: 21804524.0\n",
      "Loss: 21799960.0\n",
      "Loss: 21796658.0\n",
      "Loss: 21793694.0\n",
      "Loss: 21791300.0\n",
      "Loss: 21789132.0\n",
      "Loss: 21788770.0\n",
      "Loss: 21785132.0\n",
      "Loss: 21784742.0\n",
      "Loss: 21781120.0\n",
      "Loss: 21779498.0\n",
      "Loss: 21776594.0\n",
      "Loss: 21775530.0\n",
      "Loss: 21772160.0\n",
      "Loss: 21769722.0\n",
      "Loss: 21765934.0\n",
      "Loss: 21762802.0\n",
      "Loss: 21760512.0\n",
      "Loss: 21756050.0\n",
      "Loss: 21753274.0\n",
      "Loss: 21748452.0\n",
      "Loss: 21744274.0\n",
      "Loss: 21738692.0\n",
      "Loss: 21734356.0\n",
      "Loss: 21729190.0\n",
      "Loss: 21721834.0\n",
      "Loss: 21715632.0\n",
      "Loss: 21710352.0\n",
      "Loss: 21701684.0\n",
      "Loss: 21694612.0\n",
      "Loss: 21685052.0\n",
      "Loss: 21675620.0\n",
      "Loss: 21665428.0\n",
      "Loss: 21653948.0\n",
      "Loss: 21642970.0\n",
      "Loss: 21630152.0\n",
      "Loss: 21616880.0\n",
      "Loss: 21607896.0\n",
      "Loss: 21587878.0\n",
      "Loss: 21575018.0\n",
      "Loss: 21551318.0\n",
      "Loss: 21535106.0\n",
      "Loss: 21513436.0\n",
      "Loss: 21492386.0\n",
      "Loss: 21492268.0\n",
      "Loss: 21451920.0\n",
      "Loss: 21430900.0\n",
      "Loss: 21415540.0\n",
      "Loss: 21387972.0\n",
      "Loss: 21379256.0\n",
      "Loss: 21343576.0\n",
      "Loss: 21329130.0\n",
      "Loss: 21294850.0\n",
      "Loss: 21272474.0\n",
      "Loss: 21246632.0\n",
      "Loss: 21222780.0\n",
      "Loss: 21197538.0\n",
      "Loss: 21175270.0\n",
      "Loss: 21172502.0\n",
      "Loss: 21123272.0\n",
      "Loss: 21096936.0\n",
      "Loss: 21070138.0\n",
      "Loss: 21064588.0\n",
      "Loss: 21021220.0\n",
      "Loss: 20992868.0\n",
      "Loss: 20974456.0\n",
      "Loss: 20964280.0\n",
      "Loss: 20910852.0\n",
      "Loss: 20884950.0\n",
      "Loss: 20855146.0\n",
      "Loss: 20846166.0\n",
      "Loss: 20798888.0\n",
      "Loss: 20775624.0\n",
      "Loss: 20784536.0\n",
      "Loss: 20734664.0\n",
      "Loss: 20687814.0\n",
      "Loss: 20661178.0\n",
      "Loss: 20632866.0\n",
      "Loss: 20601912.0\n",
      "Loss: 20592004.0\n",
      "Loss: 20548198.0\n",
      "Loss: 20521976.0\n",
      "Loss: 20505626.0\n",
      "Loss: 20466118.0\n",
      "Loss: 20432048.0\n",
      "Loss: 20402388.0\n",
      "Loss: 20380760.0\n",
      "Loss: 20359612.0\n",
      "Loss: 20330786.0\n",
      "Loss: 20289314.0\n",
      "Loss: 20285272.0\n",
      "Loss: 20282258.0\n",
      "Loss: 20257614.0\n",
      "Loss: 20195528.0\n",
      "Loss: 20153404.0\n",
      "Loss: 20130380.0\n",
      "Loss: 20101976.0\n",
      "Loss: 20073598.0\n",
      "Loss: 20051444.0\n",
      "Loss: 20016780.0\n",
      "Loss: 19993510.0\n",
      "Loss: 19967984.0\n",
      "Loss: 19934786.0\n",
      "Loss: 19931634.0\n",
      "Loss: 19892060.0\n",
      "Loss: 19855792.0\n",
      "Loss: 19834904.0\n",
      "Loss: 19811398.0\n",
      "Loss: 19781076.0\n",
      "Loss: 19760840.0\n",
      "Loss: 19823394.0\n",
      "Loss: 19701830.0\n",
      "Loss: 19679964.0\n",
      "Loss: 19675476.0\n",
      "Loss: 19688342.0\n",
      "Loss: 19640916.0\n",
      "Loss: 19586018.0\n",
      "Loss: 19580122.0\n",
      "Loss: 19692202.0\n",
      "Loss: 19522594.0\n",
      "Loss: 19497346.0\n",
      "Loss: 19463206.0\n",
      "Loss: 19451282.0\n",
      "Loss: 19427596.0\n",
      "Loss: 19562270.0\n",
      "Loss: 19388544.0\n",
      "Loss: 19368774.0\n",
      "Loss: 19337342.0\n",
      "Loss: 19318658.0\n",
      "Loss: 19305318.0\n",
      "Loss: 19327816.0\n",
      "Loss: 19243934.0\n",
      "Loss: 19226802.0\n",
      "Loss: 19207892.0\n",
      "Loss: 19181890.0\n",
      "Loss: 19267910.0\n",
      "Loss: 19148968.0\n",
      "Loss: 19131710.0\n",
      "Loss: 19103326.0\n",
      "Loss: 19094174.0\n",
      "Loss: 19070302.0\n",
      "Loss: 19055678.0\n",
      "Loss: 19015794.0\n",
      "Loss: 19437990.0\n",
      "Loss: 19140394.0\n",
      "Loss: 19010160.0\n",
      "Loss: 18960536.0\n",
      "Loss: 19011076.0\n",
      "Loss: 18939354.0\n",
      "Loss: 18917138.0\n",
      "Loss: 18893064.0\n",
      "Loss: 18918294.0\n",
      "Loss: 18890586.0\n",
      "Loss: 18829646.0\n",
      "Loss: 18807744.0\n",
      "Loss: 18809956.0\n",
      "Loss: 18793552.0\n",
      "Loss: 18936132.0\n",
      "Loss: 18788878.0\n",
      "Loss: 19290812.0\n",
      "Loss: 18817676.0\n",
      "Loss: 18734772.0\n",
      "Loss: 18685634.0\n",
      "Loss: 18683350.0\n",
      "Loss: 18662760.0\n",
      "Loss: 18645268.0\n",
      "Loss: 19231848.0\n",
      "Loss: 18628164.0\n",
      "Loss: 18585260.0\n",
      "Loss: 18602804.0\n",
      "Loss: 18574616.0\n",
      "Loss: 18547410.0\n",
      "Loss: 18659904.0\n",
      "Loss: 18521136.0\n",
      "Loss: 18501934.0\n",
      "Loss: 18651538.0\n",
      "Loss: 18485838.0\n",
      "Loss: 18453320.0\n",
      "Loss: 18428734.0\n",
      "Loss: 18417294.0\n",
      "Loss: 18401746.0\n",
      "Loss: 18436072.0\n",
      "Loss: 18448140.0\n",
      "Loss: 18352396.0\n",
      "Loss: 18365858.0\n",
      "Loss: 18316972.0\n",
      "Loss: 18438670.0\n",
      "Loss: 18300056.0\n",
      "Loss: 18327924.0\n",
      "Loss: 18284878.0\n",
      "Loss: 18295282.0\n",
      "Loss: 18254206.0\n",
      "Loss: 18230300.0\n",
      "Loss: 18208732.0\n",
      "Loss: 18191228.0\n",
      "Loss: 18196152.0\n",
      "Loss: 18177602.0\n",
      "Loss: 18184672.0\n",
      "Loss: 18153680.0\n",
      "Loss: 18145104.0\n",
      "Loss: 18127842.0\n",
      "Loss: 18128710.0\n",
      "Loss: 18096578.0\n",
      "Loss: 18085578.0\n",
      "Loss: 18114416.0\n",
      "Loss: 18120584.0\n",
      "Loss: 18036102.0\n",
      "Loss: 18033636.0\n",
      "Loss: 18000296.0\n",
      "Loss: 18042120.0\n",
      "Loss: 17982224.0\n",
      "Loss: 17965236.0\n",
      "Loss: 17960382.0\n",
      "Loss: 17972324.0\n",
      "Loss: 17933908.0\n",
      "Loss: 17943368.0\n",
      "Loss: 17908566.0\n",
      "Loss: 17943204.0\n",
      "Loss: 17877820.0\n",
      "Loss: 17911084.0\n",
      "Loss: 17862164.0\n",
      "Loss: 17827038.0\n",
      "Loss: 17821326.0\n",
      "Loss: 17858908.0\n",
      "Loss: 17792028.0\n",
      "Loss: 17781706.0\n",
      "Loss: 17814884.0\n",
      "Loss: 17747314.0\n",
      "Loss: 17792076.0\n",
      "Loss: 17738236.0\n",
      "Loss: 17717454.0\n",
      "Loss: 17796492.0\n",
      "Loss: 17736066.0\n",
      "Loss: 17755656.0\n",
      "Loss: 17702542.0\n",
      "Loss: 17672646.0\n",
      "Loss: 17691096.0\n",
      "Loss: 17654096.0\n",
      "Loss: 17737868.0\n",
      "Loss: 17639254.0\n",
      "Loss: 17637618.0\n",
      "Loss: 17601918.0\n",
      "Loss: 17652954.0\n",
      "Loss: 17565116.0\n",
      "Loss: 17554098.0\n",
      "Loss: 17806928.0\n",
      "Loss: 17527382.0\n",
      "Loss: 17583870.0\n",
      "Loss: 17528682.0\n",
      "Loss: 17525472.0\n",
      "Loss: 17477816.0\n",
      "Loss: 17570514.0\n",
      "Loss: 17479416.0\n",
      "Loss: 17594950.0\n",
      "Loss: 17440614.0\n",
      "Loss: 17503738.0\n",
      "Loss: 17413268.0\n",
      "Loss: 17415994.0\n",
      "Loss: 17595182.0\n",
      "Loss: 17385654.0\n",
      "Loss: 17360798.0\n",
      "Loss: 17674034.0\n",
      "Loss: 17349276.0\n",
      "Loss: 17343970.0\n",
      "Loss: 17315790.0\n",
      "Loss: 17294854.0\n",
      "Loss: 17620004.0\n",
      "Loss: 17269476.0\n",
      "Loss: 17278818.0\n",
      "Loss: 17329398.0\n",
      "Loss: 17624610.0\n",
      "Loss: 17313362.0\n",
      "Loss: 17564036.0\n",
      "Loss: 17287620.0\n",
      "Loss: 17225520.0\n",
      "Loss: 17202034.0\n",
      "Loss: 17215116.0\n",
      "Loss: 17443464.0\n",
      "Loss: 17194092.0\n",
      "Loss: 17192346.0\n",
      "Loss: 17140022.0\n",
      "Loss: 17147450.0\n",
      "Loss: 17135632.0\n",
      "Loss: 17103538.0\n",
      "Loss: 17086512.0\n",
      "Loss: 17097038.0\n",
      "Loss: 17114966.0\n",
      "Loss: 17092552.0\n",
      "Loss: 17229350.0\n",
      "Loss: 17071318.0\n",
      "Loss: 17087884.0\n",
      "Loss: 17051966.0\n",
      "Loss: 17054744.0\n",
      "Loss: 17029668.0\n",
      "Loss: 17343966.0\n",
      "Loss: 16997504.0\n",
      "Loss: 17348474.0\n",
      "Loss: 16975746.0\n",
      "Loss: 17208498.0\n",
      "Loss: 16950434.0\n",
      "Loss: 16929526.0\n",
      "Loss: 16975608.0\n",
      "Loss: 17381554.0\n",
      "Loss: 17322002.0\n",
      "Loss: 16903576.0\n",
      "Loss: 16915048.0\n",
      "Loss: 16888276.0\n",
      "Loss: 17197102.0\n",
      "Loss: 16894926.0\n",
      "Loss: 16868068.0\n",
      "Loss: 16862086.0\n",
      "Loss: 16880296.0\n",
      "Loss: 16791858.0\n",
      "Loss: 16790668.0\n",
      "Loss: 16834382.0\n",
      "Loss: 16784286.0\n",
      "Loss: 16762845.0\n",
      "Loss: 16802268.0\n",
      "Loss: 16739497.0\n",
      "Loss: 16740614.0\n",
      "Loss: 16773806.0\n",
      "Loss: 16719267.0\n",
      "Loss: 16680661.0\n",
      "Loss: 16680047.0\n",
      "Loss: 16710962.0\n",
      "Loss: 16695787.0\n",
      "Loss: 16688348.0\n",
      "Loss: 16918770.0\n",
      "Loss: 16741045.0\n",
      "Loss: 16680756.0\n",
      "Loss: 16631253.0\n",
      "Loss: 16606681.0\n",
      "Loss: 16609178.0\n",
      "Loss: 16586729.0\n",
      "Loss: 16580371.0\n",
      "Loss: 16574903.0\n",
      "Loss: 16554601.0\n",
      "Loss: 18466388.0\n",
      "Loss: 16643009.0\n",
      "Loss: 16939302.0\n",
      "Loss: 16591998.0\n",
      "Loss: 16504854.0\n",
      "Loss: 16501613.0\n",
      "Loss: 16508887.0\n",
      "Loss: 16486758.0\n",
      "Loss: 16502365.0\n",
      "Loss: 16600737.0\n",
      "Loss: 16461460.0\n",
      "Loss: 16438333.0\n",
      "Loss: 16430850.0\n",
      "Loss: 16432476.0\n",
      "Loss: 16522727.0\n",
      "Loss: 16753644.0\n",
      "Loss: 16397465.0\n",
      "Loss: 16382246.0\n",
      "Loss: 16396074.0\n",
      "Loss: 16373772.0\n",
      "Loss: 16806726.0\n",
      "Loss: 16456916.0\n",
      "Loss: 16324500.0\n",
      "Loss: 16328073.0\n",
      "Loss: 16312026.0\n",
      "Loss: 16296408.0\n",
      "Loss: 16321131.0\n",
      "Loss: 16262229.0\n",
      "Loss: 16290751.0\n",
      "Loss: 16304284.0\n",
      "Loss: 16287780.0\n",
      "Loss: 16514447.0\n",
      "Loss: 16276668.0\n",
      "Loss: 16238054.0\n",
      "Loss: 16228618.0\n",
      "Loss: 16208771.0\n",
      "Loss: 16192591.0\n",
      "Loss: 16182055.0\n",
      "Loss: 16153069.0\n",
      "Loss: 16248831.0\n",
      "Loss: 16183069.0\n",
      "Loss: 16280132.0\n",
      "Loss: 16133256.0\n",
      "Loss: 16123564.0\n",
      "Loss: 16102617.0\n",
      "Loss: 16107125.0\n",
      "Loss: 16079181.0\n",
      "Loss: 16341092.0\n",
      "Loss: 16101966.0\n",
      "Loss: 16354260.0\n",
      "Loss: 16039377.0\n",
      "Loss: 16022097.0\n",
      "Loss: 16004207.0\n",
      "Loss: 16029594.0\n",
      "Loss: 16031718.0\n",
      "Loss: 15977039.0\n",
      "Loss: 111208624.0\n",
      "Loss: 16588244.0\n",
      "Loss: 16091972.0\n",
      "Loss: 16041559.0\n",
      "Loss: 16000941.0\n",
      "Loss: 15920011.0\n",
      "Loss: 15924310.0\n",
      "Loss: 15895229.0\n",
      "Loss: 15911992.0\n",
      "Loss: 15866319.0\n",
      "Loss: 15818259.0\n",
      "Loss: 18338458.0\n",
      "Loss: 15844130.0\n",
      "Loss: 15829487.0\n",
      "Loss: 15804794.0\n",
      "Loss: 15786222.0\n",
      "Loss: 15770650.0\n",
      "Loss: 16068323.0\n",
      "Loss: 18299972.0\n",
      "Loss: 15739395.0\n",
      "Loss: 15759538.0\n",
      "Loss: 15739680.0\n",
      "Loss: 15782915.0\n",
      "Loss: 15744661.0\n",
      "Loss: 15735954.0\n",
      "Loss: 15692392.0\n",
      "Loss: 15894655.0\n",
      "Loss: 15845762.0\n",
      "Loss: 15694481.0\n",
      "Loss: 15950057.0\n",
      "Loss: 15678665.0\n",
      "Loss: 15637917.0\n",
      "Loss: 15644159.0\n",
      "Loss: 15634580.0\n",
      "Loss: 15760652.0\n",
      "Loss: 15610573.0\n",
      "Loss: 15643467.0\n",
      "Loss: 15632172.0\n",
      "Loss: 15575455.0\n",
      "Loss: 15590336.0\n",
      "Loss: 15575671.0\n",
      "Loss: 15592406.0\n",
      "Loss: 15582966.0\n",
      "Loss: 15537150.0\n",
      "Loss: 15525024.0\n",
      "Loss: 15531937.0\n",
      "Loss: 15517240.0\n",
      "Loss: 15501939.0\n",
      "Loss: 15509498.0\n",
      "Loss: 15500620.0\n",
      "Loss: 15481389.0\n",
      "Loss: 15821116.0\n",
      "Loss: 15500862.0\n",
      "Loss: 15460260.0\n",
      "Loss: 15465423.0\n",
      "Loss: 15499353.0\n",
      "Loss: 15437480.0\n",
      "Loss: 15419839.0\n",
      "Loss: 15635449.0\n",
      "Loss: 15407358.0\n",
      "Loss: 15371819.0\n",
      "Loss: 15376815.0\n",
      "Loss: 15336743.0\n",
      "Loss: 15346291.0\n",
      "Loss: 15333501.0\n",
      "Loss: 15321567.0\n",
      "Loss: 15338003.0\n",
      "Loss: 15293213.0\n",
      "Loss: 15347455.0\n",
      "Loss: 15305378.0\n",
      "Loss: 16121188.0\n",
      "Loss: 15306690.0\n",
      "Loss: 15265054.0\n",
      "Loss: 15266795.0\n",
      "Loss: 15310696.0\n",
      "Loss: 15240354.0\n",
      "Loss: 15724303.0\n",
      "Loss: 15228172.0\n",
      "Loss: 15237212.0\n",
      "Loss: 103297248.0\n",
      "Loss: 15499286.0\n",
      "Loss: 15396883.0\n",
      "Loss: 15266377.0\n",
      "Loss: 15174814.0\n",
      "Loss: 15156162.0\n",
      "Loss: 15121330.0\n",
      "Loss: 15090959.0\n",
      "Loss: 15084873.0\n",
      "Loss: 15065638.0\n",
      "Loss: 15091861.0\n",
      "Loss: 15109586.0\n",
      "Loss: 15086233.0\n",
      "Loss: 15028522.0\n",
      "Loss: 15048256.0\n",
      "Loss: 15049807.0\n",
      "Loss: 15023341.0\n",
      "Loss: 15324009.0\n",
      "Loss: 15006208.0\n",
      "Loss: 15016643.0\n",
      "Loss: 14968663.0\n",
      "Loss: 14981722.0\n",
      "Loss: 14981692.0\n",
      "Loss: 14957314.0\n",
      "Loss: 14968224.0\n",
      "Loss: 14955474.0\n",
      "Loss: 14917532.0\n",
      "Loss: 14966697.0\n",
      "Loss: 14985545.0\n",
      "Loss: 14980227.0\n",
      "Loss: 14922462.0\n",
      "Loss: 14887649.0\n",
      "Loss: 14904266.0\n",
      "Loss: 14876929.0\n",
      "Loss: 14903705.0\n",
      "Loss: 14928977.0\n",
      "Loss: 14818880.0\n",
      "Loss: 14832354.0\n",
      "Loss: 14827235.0\n",
      "Loss: 14879732.0\n",
      "Loss: 14817699.0\n",
      "Loss: 14760515.0\n",
      "Loss: 14819990.0\n",
      "Loss: 14774677.0\n",
      "Loss: 14731041.0\n",
      "Loss: 14747982.0\n",
      "Loss: 14726186.0\n",
      "Loss: 14729163.0\n",
      "Loss: 14695927.0\n",
      "Loss: 97589336.0\n",
      "Loss: 14874233.0\n",
      "Loss: 14821818.0\n",
      "Loss: 14764946.0\n",
      "Loss: 14891064.0\n",
      "Loss: 14690205.0\n",
      "Loss: 14640140.0\n",
      "Loss: 18475774.0\n",
      "Loss: 14621142.0\n",
      "Loss: 14943555.0\n",
      "Loss: 14676658.0\n",
      "Loss: 14589602.0\n",
      "Loss: 14552177.0\n",
      "Loss: 14564470.0\n",
      "Loss: 14561255.0\n",
      "Loss: 14589553.0\n",
      "Loss: 14536826.0\n",
      "Loss: 14516206.0\n",
      "Loss: 14545820.0\n",
      "Loss: 15085001.0\n",
      "Loss: 14512867.0\n",
      "Loss: 14485483.0\n",
      "Loss: 14579711.0\n",
      "Loss: 14462731.0\n",
      "Loss: 14664188.0\n",
      "Loss: 14451473.0\n",
      "Loss: 14467295.0\n",
      "Loss: 14447235.0\n",
      "Loss: 14559896.0\n",
      "Loss: 14533414.0\n",
      "Loss: 14420852.0\n",
      "Loss: 14417274.0\n",
      "Loss: 14419593.0\n",
      "Loss: 14435784.0\n",
      "Loss: 14377676.0\n",
      "Loss: 14383143.0\n",
      "Loss: 14348000.0\n",
      "Loss: 14338557.0\n",
      "Loss: 14379554.0\n",
      "Loss: 14587338.0\n",
      "Loss: 14313110.0\n",
      "Loss: 14343631.0\n",
      "Loss: 14343487.0\n",
      "Loss: 14384951.0\n",
      "Loss: 14382419.0\n",
      "Loss: 14332822.0\n",
      "Loss: 14283624.0\n",
      "Loss: 14267027.0\n",
      "Loss: 14249262.0\n",
      "Loss: 14280132.0\n",
      "Loss: 14240526.0\n",
      "Loss: 14231084.0\n",
      "Loss: 14228669.0\n",
      "Loss: 14292094.0\n",
      "Loss: 14294023.0\n",
      "Loss: 14198494.0\n",
      "Loss: 14251939.0\n",
      "Loss: 14209441.0\n",
      "Loss: 14212790.0\n",
      "Loss: 14239448.0\n",
      "Loss: 14159684.0\n",
      "Loss: 14147186.0\n",
      "Loss: 14220913.0\n",
      "Loss: 14251155.0\n",
      "Loss: 14153367.0\n",
      "Loss: 14169302.0\n",
      "Loss: 14845582.0\n",
      "Loss: 14133581.0\n",
      "Loss: 14146994.0\n",
      "Loss: 14086502.0\n",
      "Loss: 14935524.0\n",
      "Loss: 14030008.0\n",
      "Loss: 14189774.0\n",
      "Loss: 14043123.0\n",
      "Loss: 14078670.0\n",
      "Loss: 14031832.0\n",
      "Loss: 14231275.0\n",
      "Loss: 14045305.0\n",
      "Loss: 14018473.0\n",
      "Loss: 14002472.0\n",
      "Loss: 14043821.0\n",
      "Loss: 13975019.0\n",
      "Loss: 13962016.0\n",
      "Loss: 13957035.0\n",
      "Loss: 13939213.0\n",
      "Loss: 13936365.0\n",
      "Loss: 13942328.0\n",
      "Loss: 13936294.0\n",
      "Loss: 14171862.0\n",
      "Loss: 13920610.0\n",
      "Loss: 13875538.0\n",
      "Loss: 13869087.0\n",
      "Loss: 13894172.0\n",
      "Loss: 13869992.0\n",
      "Loss: 13916532.0\n",
      "Loss: 14216896.0\n",
      "Loss: 13856302.0\n",
      "Loss: 13929897.0\n",
      "Loss: 13834151.0\n",
      "Loss: 13815362.0\n",
      "Loss: 13802048.0\n",
      "Loss: 13873769.0\n",
      "Loss: 13847294.0\n",
      "Loss: 13764760.0\n",
      "Loss: 13793265.0\n",
      "Loss: 13755812.0\n",
      "Loss: 13686498.0\n",
      "Loss: 13714575.0\n",
      "Loss: 13706110.0\n",
      "Loss: 13846857.0\n",
      "Loss: 13813565.0\n",
      "Loss: 13671836.0\n",
      "Loss: 13658401.0\n",
      "Loss: 13779594.0\n",
      "Loss: 13661891.0\n",
      "Loss: 13661525.0\n",
      "Loss: 13670048.0\n",
      "Loss: 13672391.0\n",
      "Loss: 13797588.0\n",
      "Loss: 13634746.0\n",
      "Loss: 14163910.0\n",
      "Loss: 13604693.0\n",
      "Loss: 13583651.0\n",
      "Loss: 13604711.0\n",
      "Loss: 13549928.0\n",
      "Loss: 13519761.0\n",
      "Loss: 13558670.0\n",
      "Loss: 13549268.0\n",
      "Loss: 13549736.0\n",
      "Loss: 13506964.0\n",
      "Loss: 13519412.0\n",
      "Loss: 13496538.0\n",
      "Loss: 13478129.0\n",
      "Loss: 13505336.0\n",
      "Loss: 13458115.0\n",
      "Loss: 13740819.0\n",
      "Loss: 13552511.0\n",
      "Loss: 13444751.0\n",
      "Loss: 13381996.0\n",
      "Loss: 13372072.0\n",
      "Loss: 13346670.0\n",
      "Loss: 13363263.0\n",
      "Loss: 13344798.0\n",
      "Loss: 13557009.0\n",
      "Loss: 13326381.0\n",
      "Loss: 13356269.0\n",
      "Loss: 13337207.0\n",
      "Loss: 13371638.0\n",
      "Loss: 13377513.0\n",
      "Loss: 13261612.0\n",
      "Loss: 13410390.0\n",
      "Loss: 13321809.0\n",
      "Loss: 13246013.0\n",
      "Loss: 13231746.0\n",
      "Loss: 13245853.0\n",
      "Loss: 13216573.0\n",
      "Loss: 13179130.0\n",
      "Loss: 13236177.0\n",
      "Loss: 13214783.0\n",
      "Loss: 13175477.0\n",
      "Loss: 13169732.0\n",
      "Loss: 13186335.0\n",
      "Loss: 13250035.0\n",
      "Loss: 13198089.0\n",
      "Loss: 13118435.0\n",
      "Loss: 13109159.0\n",
      "Loss: 13129846.0\n",
      "Loss: 13091218.0\n",
      "Loss: 13323361.0\n",
      "Loss: 13123223.0\n",
      "Loss: 13082336.0\n",
      "Loss: 13095964.0\n",
      "Loss: 13067190.0\n",
      "Loss: 13072734.0\n",
      "Loss: 13204853.0\n",
      "Loss: 13096079.0\n",
      "Loss: 13078778.0\n",
      "Loss: 13022813.0\n",
      "Loss: 12960258.0\n",
      "Loss: 12985875.0\n",
      "Loss: 13022926.0\n",
      "Loss: 13133965.0\n",
      "Loss: 12969738.0\n",
      "Loss: 12945394.0\n",
      "Loss: 12923190.0\n",
      "Loss: 13398445.0\n",
      "Loss: 13029090.0\n",
      "Loss: 13039938.0\n",
      "Loss: 12896572.0\n",
      "Loss: 12852032.0\n",
      "Loss: 12950825.0\n",
      "Loss: 12848243.0\n",
      "Loss: 12862726.0\n",
      "Loss: 12820441.0\n",
      "Loss: 12987821.0\n",
      "Loss: 12842250.0\n",
      "Loss: 12831460.0\n",
      "Loss: 12817579.0\n",
      "Loss: 12869254.0\n",
      "Loss: 12823347.0\n",
      "Loss: 12800423.0\n",
      "Loss: 12757228.0\n",
      "Loss: 12775847.0\n",
      "Loss: 12731746.0\n",
      "Loss: 12724878.0\n",
      "Loss: 12834498.0\n",
      "Loss: 12699255.0\n",
      "Loss: 12734002.0\n",
      "Loss: 12660317.0\n",
      "Loss: 12667185.0\n",
      "Loss: 12682025.0\n",
      "Loss: 12629272.0\n",
      "Loss: 12706873.0\n",
      "Loss: 12627267.0\n",
      "Loss: 12725409.0\n",
      "Loss: 12590613.0\n",
      "Loss: 12594091.0\n",
      "Loss: 12635366.0\n",
      "Loss: 12614999.0\n",
      "Loss: 12582802.0\n",
      "Loss: 12563567.0\n",
      "Loss: 12587475.0\n",
      "Loss: 12548132.0\n",
      "Loss: 12549468.0\n",
      "Loss: 12498099.0\n",
      "Loss: 12501907.0\n",
      "Loss: 12493504.0\n",
      "Loss: 12452553.0\n",
      "Loss: 12491901.0\n",
      "Loss: 12486175.0\n",
      "Loss: 12487885.0\n",
      "Loss: 12460493.0\n",
      "Loss: 12424068.0\n",
      "Loss: 12461732.0\n",
      "Loss: 12542499.0\n",
      "Loss: 12522070.0\n",
      "Loss: 12422207.0\n",
      "Loss: 12400968.0\n",
      "Loss: 12431473.0\n",
      "Loss: 12405294.0\n",
      "Loss: 12371031.0\n",
      "Loss: 12359260.0\n",
      "Loss: 12426976.0\n",
      "Loss: 12363129.0\n",
      "Loss: 12299624.0\n",
      "Loss: 12332108.0\n",
      "Loss: 12292759.0\n",
      "Loss: 12311623.0\n",
      "Loss: 12264865.0\n",
      "Loss: 12387119.0\n",
      "Loss: 12319041.0\n",
      "Loss: 12393854.0\n",
      "Loss: 12342250.0\n",
      "Loss: 12346972.0\n",
      "Loss: 12199842.0\n",
      "Loss: 12196510.0\n",
      "Loss: 12344584.0\n",
      "Loss: 12493916.0\n",
      "Loss: 12244365.0\n",
      "Loss: 12179740.0\n",
      "Loss: 12157303.0\n",
      "Loss: 12149000.0\n",
      "Loss: 12190660.0\n",
      "Loss: 12116547.0\n",
      "Loss: 12148579.0\n",
      "Loss: 12142566.0\n",
      "Loss: 12164118.0\n",
      "Loss: 12118910.0\n",
      "Loss: 17563336.0\n",
      "Loss: 11973666.0\n",
      "Loss: 12104865.0\n",
      "Loss: 12043968.0\n",
      "Loss: 12113973.0\n",
      "Loss: 12184665.0\n",
      "Loss: 12075283.0\n",
      "Loss: 12064529.0\n",
      "Loss: 12037217.0\n",
      "Loss: 12010948.0\n",
      "Loss: 12017583.0\n",
      "Loss: 11990016.0\n",
      "Loss: 12071489.0\n",
      "Loss: 11965963.0\n",
      "Loss: 11922773.0\n",
      "Loss: 11926741.0\n",
      "Loss: 11919797.0\n",
      "Loss: 11903005.0\n",
      "Loss: 11977785.0\n",
      "Loss: 11872438.0\n",
      "Loss: 11941034.0\n",
      "Loss: 11894589.0\n",
      "Loss: 11914351.0\n",
      "Loss: 11901085.0\n",
      "Loss: 11861449.0\n",
      "Loss: 11840354.0\n",
      "Loss: 11865471.0\n",
      "Loss: 11818398.0\n",
      "Loss: 11791524.0\n",
      "Loss: 11833206.0\n",
      "Loss: 11771111.0\n",
      "Loss: 11823178.0\n",
      "Loss: 11877777.0\n",
      "Loss: 11987913.0\n",
      "Loss: 11947148.0\n",
      "Loss: 11746660.0\n",
      "Loss: 11818610.0\n",
      "Loss: 11790418.0\n",
      "Loss: 11736157.0\n",
      "Loss: 11705973.0\n",
      "Loss: 11696340.0\n",
      "Loss: 11769366.0\n",
      "Loss: 11698676.0\n",
      "Loss: 11684050.0\n",
      "Loss: 11691165.0\n",
      "Loss: 11658293.0\n",
      "Loss: 11648032.0\n",
      "Loss: 11692708.0\n",
      "Loss: 11668725.0\n",
      "Loss: 11797079.0\n",
      "Loss: 11658836.0\n",
      "Loss: 11574161.0\n",
      "Loss: 11565511.0\n",
      "Loss: 11640286.0\n",
      "Loss: 11688209.0\n",
      "Loss: 11668099.0\n",
      "Loss: 11568161.0\n",
      "Loss: 11532804.0\n",
      "Loss: 11537218.0\n",
      "Loss: 11567814.0\n",
      "Loss: 11558094.0\n",
      "Loss: 11522899.0\n",
      "Loss: 11538766.0\n",
      "Loss: 11547217.0\n",
      "Loss: 68822696.0\n",
      "Loss: 12226250.0\n",
      "Loss: 11635618.0\n",
      "Loss: 11576031.0\n",
      "Loss: 11498776.0\n",
      "Loss: 11507912.0\n",
      "Loss: 11463974.0\n",
      "Loss: 11436242.0\n",
      "Loss: 11409822.0\n",
      "Loss: 11382494.0\n",
      "Loss: 11390515.0\n",
      "Loss: 11341331.0\n",
      "Loss: 11371776.0\n",
      "Loss: 11362316.0\n",
      "Loss: 11334413.0\n",
      "Loss: 11368595.0\n",
      "Loss: 11360939.0\n",
      "Loss: 11368348.0\n",
      "Loss: 11303401.0\n",
      "Loss: 11286842.0\n",
      "Loss: 64991740.0\n",
      "Loss: 12108717.0\n",
      "Loss: 11464957.0\n",
      "Loss: 11482281.0\n",
      "Loss: 11385916.0\n",
      "Loss: 11365721.0\n",
      "Loss: 11304404.0\n",
      "Loss: 11325999.0\n",
      "Loss: 11323526.0\n",
      "Loss: 11236941.0\n",
      "Loss: 11433714.0\n",
      "Loss: 11217316.0\n",
      "Loss: 11223416.0\n",
      "Loss: 11197479.0\n",
      "Loss: 11164523.0\n",
      "Loss: 11238318.0\n",
      "Loss: 11217132.0\n",
      "Loss: 11170765.0\n",
      "Loss: 11155974.0\n",
      "Loss: 11170221.0\n",
      "Loss: 11131456.0\n",
      "Loss: 11167754.0\n",
      "Loss: 11139794.0\n",
      "Loss: 11185707.0\n",
      "Loss: 11146918.0\n",
      "Loss: 11092501.0\n",
      "Loss: 11267770.0\n",
      "Loss: 11114521.0\n",
      "Loss: 11087669.0\n",
      "Loss: 11073406.0\n",
      "Loss: 11051319.0\n",
      "Loss: 11093756.0\n",
      "Loss: 11106795.0\n",
      "Loss: 11050581.0\n",
      "Loss: 11152247.0\n",
      "Loss: 11005629.0\n",
      "Loss: 11019509.0\n",
      "Loss: 11046122.0\n",
      "Loss: 11016012.0\n",
      "Loss: 11080071.0\n",
      "Loss: 11011020.0\n",
      "Loss: 10998404.0\n",
      "Loss: 11003866.0\n",
      "Loss: 10968602.0\n",
      "Loss: 10982833.0\n",
      "Loss: 11086733.0\n",
      "Loss: 10943914.0\n",
      "Loss: 10975655.0\n",
      "Loss: 10980060.0\n",
      "Loss: 10939021.0\n",
      "Loss: 10977240.0\n",
      "Loss: 10920106.0\n",
      "Loss: 11016481.0\n",
      "Loss: 10905146.0\n",
      "Loss: 10927214.0\n",
      "Loss: 10922313.0\n",
      "Loss: 10895389.0\n",
      "Loss: 10881000.0\n",
      "Loss: 10936063.0\n",
      "Loss: 11007680.0\n",
      "Loss: 10838541.0\n",
      "Loss: 10879999.0\n",
      "Loss: 10850364.0\n",
      "Loss: 10858742.0\n",
      "Loss: 10783698.0\n",
      "Loss: 11334279.0\n",
      "Loss: 10877908.0\n",
      "Loss: 10837012.0\n",
      "Loss: 10794834.0\n",
      "Loss: 10774056.0\n",
      "Loss: 10743046.0\n",
      "Loss: 10757198.0\n",
      "Loss: 10763635.0\n",
      "Loss: 10740676.0\n",
      "Loss: 17753560.0\n",
      "Loss: 10814029.0\n",
      "Loss: 10818827.0\n",
      "Loss: 10741069.0\n",
      "Loss: 10728647.0\n",
      "Loss: 10712362.0\n",
      "Loss: 10764137.0\n",
      "Loss: 10695022.0\n",
      "Loss: 10707676.0\n",
      "Loss: 10711375.0\n",
      "Loss: 10795907.0\n",
      "Loss: 10665825.0\n",
      "Loss: 10613463.0\n",
      "Loss: 10653272.0\n",
      "Loss: 10656722.0\n",
      "Loss: 10670173.0\n",
      "Loss: 10731700.0\n",
      "Loss: 10622374.0\n",
      "Loss: 10601898.0\n",
      "Loss: 10601988.0\n",
      "Loss: 10644817.0\n",
      "Loss: 10618413.0\n",
      "Loss: 10609398.0\n",
      "Loss: 10580856.0\n",
      "Loss: 10589373.0\n",
      "Loss: 10606635.0\n",
      "Loss: 10522803.0\n",
      "Loss: 10521153.0\n",
      "Loss: 10479795.0\n",
      "Loss: 10547535.0\n",
      "Loss: 10610761.0\n",
      "Loss: 10515876.0\n",
      "Loss: 10491770.0\n",
      "Loss: 10487039.0\n",
      "Loss: 10516105.0\n",
      "Loss: 10477871.0\n",
      "Loss: 10497148.0\n",
      "Loss: 10443361.0\n",
      "Loss: 10508095.0\n",
      "Loss: 18013382.0\n",
      "Loss: 10516470.0\n",
      "Loss: 10508094.0\n",
      "Loss: 10413220.0\n",
      "Loss: 10405987.0\n",
      "Loss: 60376008.0\n",
      "Loss: 10351633.0\n",
      "Loss: 10441523.0\n",
      "Loss: 10445555.0\n",
      "Loss: 10396929.0\n",
      "Testing model\n",
      "Done. Test loss: 628140.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isidre/Documents/UAB/TFM/FootballAnalyticsTFM/.venv1/lib/python3.8/site-packages/spektral/layers/convolutional/conv.py:93: UserWarning: The adjacency matrix of dtype <dtype: 'int64'> is incompatible with the dtype of the node features <dtype: 'float32'> and has been automatically cast to <dtype: 'float32'>.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example shows how to perform regression of molecular properties with the\n",
    "QM9 database, using a simple GNN in disjoint mode.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data import DisjointLoader\n",
    "from spektral.datasets import QM9\n",
    "from spektral.layers import ECCConv, GlobalSumPool\n",
    "\n",
    "################################################################################\n",
    "# Config\n",
    "################################################################################\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "epochs = 1000  # Number of training epochs\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "################################################################################\n",
    "# Load data\n",
    "################################################################################\n",
    "dataset = QM9(amount=1000)  # Set amount=None to train on whole dataset\n",
    "\n",
    "# Parameters\n",
    "F = dataset.n_node_features  # Dimension of node features\n",
    "S = dataset.n_edge_features  # Dimension of edge features\n",
    "n_out = dataset.n_labels  # Dimension of the target\n",
    "\n",
    "# Train/test split\n",
    "idxs = np.random.permutation(len(dataset))\n",
    "split = int(0.9 * len(dataset))\n",
    "idx_tr, idx_te = np.split(idxs, [split])\n",
    "dataset_tr, dataset_te = dataset[idx_tr], dataset[idx_te]\n",
    "\n",
    "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
    "\n",
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "class Net(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ECCConv(32, activation=\"relu\")\n",
    "        self.conv2 = ECCConv(32, activation=\"relu\")\n",
    "        self.global_pool = GlobalSumPool()\n",
    "        self.dense = Dense(n_out)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, e, i = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        x = self.conv2([x, a, e])\n",
    "        output = self.global_pool([x, i])\n",
    "        output = self.dense(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Net()\n",
    "optimizer = Adam(learning_rate)\n",
    "loss_fn = MeanSquaredError()\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Fit model\n",
    "################################################################################\n",
    "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "step = loss = 0\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss += train_step(*batch)\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
    "        loss = 0\n",
    "\n",
    "################################################################################\n",
    "# Evaluate model\n",
    "################################################################################\n",
    "print(\"Testing model\")\n",
    "loss = 0\n",
    "for batch in loader_te:\n",
    "    inputs, target = batch\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss += loss_fn(target, predictions)\n",
    "loss /= loader_te.steps_per_epoch\n",
    "print(\"Done. Test loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "deae8ace-922e-42cd-b2e9-572e5da96608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 0)\t1\n",
      "  (3, 0)\t1\n",
      "  (4, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba18f2-ea47-4d23-9d34-893ce1aaf476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv1': venv)",
   "language": "python",
   "name": "python3810jvsc74a57bd06e0aed3ed461d30436457c9d897a6620dc7a130baf50f7322b8852eb4291a86a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
