{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5af6db94-c3eb-466c-9b40-7d6e43d97e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QM9 dataset.\n",
      "Reading SDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 1356.29it/s]\n",
      "2022-05-10 17:26:15.746034: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-10 17:26:15.747822: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/isidre/Documents/UAB/TFM/FootballAnalyticsTFM/.venv1/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:390: UserWarning: The adjacency matrix of dtype <dtype: 'int64'> is incompatible with the dtype of the node features <dtype: 'float32'> and has been automatically cast to <dtype: 'float32'>.\n",
      "  return py_builtins.overload_of(f)(*args)\n",
      "2022-05-10 17:26:16.593152: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 22110782.0\n",
      "Loss: 22047232.0\n",
      "Loss: 21904606.0\n",
      "Loss: 21820092.0\n",
      "Loss: 21806356.0\n",
      "Loss: 21798554.0\n",
      "Loss: 21795468.0\n",
      "Loss: 21792614.0\n",
      "Loss: 21790482.0\n",
      "Loss: 21788376.0\n",
      "Loss: 21786768.0\n",
      "Loss: 21784460.0\n",
      "Loss: 21783374.0\n",
      "Loss: 21780754.0\n",
      "Loss: 21778796.0\n",
      "Loss: 21776766.0\n",
      "Loss: 21774330.0\n",
      "Loss: 21772702.0\n",
      "Loss: 21769612.0\n",
      "Loss: 21767686.0\n",
      "Loss: 21764206.0\n",
      "Loss: 21761484.0\n",
      "Loss: 21758498.0\n",
      "Loss: 21754880.0\n",
      "Loss: 21750898.0\n",
      "Loss: 21746402.0\n",
      "Loss: 21742400.0\n",
      "Loss: 21738032.0\n",
      "Loss: 21732740.0\n",
      "Loss: 21727154.0\n",
      "Loss: 21722100.0\n",
      "Loss: 21718780.0\n",
      "Loss: 21708594.0\n",
      "Loss: 21700858.0\n",
      "Loss: 21693476.0\n",
      "Loss: 21683352.0\n",
      "Loss: 21678882.0\n",
      "Loss: 21664588.0\n",
      "Loss: 21653590.0\n",
      "Loss: 21641578.0\n",
      "Loss: 21628656.0\n",
      "Loss: 21616408.0\n",
      "Loss: 21602818.0\n",
      "Loss: 21586540.0\n",
      "Loss: 21572176.0\n",
      "Loss: 21554942.0\n",
      "Loss: 21537556.0\n",
      "Loss: 21519446.0\n",
      "Loss: 21501652.0\n",
      "Loss: 21482152.0\n",
      "Loss: 21468254.0\n",
      "Loss: 21442914.0\n",
      "Loss: 21422022.0\n",
      "Loss: 21400422.0\n",
      "Loss: 21376618.0\n",
      "Loss: 21357556.0\n",
      "Loss: 21333752.0\n",
      "Loss: 21305578.0\n",
      "Loss: 21282470.0\n",
      "Loss: 21258430.0\n",
      "Loss: 21231082.0\n",
      "Loss: 21219788.0\n",
      "Loss: 21182686.0\n",
      "Loss: 21155068.0\n",
      "Loss: 168552208.0\n",
      "Loss: 21235052.0\n",
      "Loss: 21029734.0\n",
      "Loss: 21011270.0\n",
      "Loss: 21008294.0\n",
      "Loss: 20980594.0\n",
      "Loss: 20967182.0\n",
      "Loss: 20956390.0\n",
      "Loss: 20939258.0\n",
      "Loss: 20930522.0\n",
      "Loss: 20938552.0\n",
      "Loss: 20912746.0\n",
      "Loss: 20880796.0\n",
      "Loss: 20866044.0\n",
      "Loss: 20851240.0\n",
      "Loss: 20834090.0\n",
      "Loss: 20818714.0\n",
      "Loss: 20802296.0\n",
      "Loss: 20783738.0\n",
      "Loss: 20765324.0\n",
      "Loss: 20769840.0\n",
      "Loss: 20731438.0\n",
      "Loss: 20722354.0\n",
      "Loss: 20695488.0\n",
      "Loss: 20677762.0\n",
      "Loss: 20655368.0\n",
      "Loss: 20646656.0\n",
      "Loss: 20623898.0\n",
      "Loss: 20616666.0\n",
      "Loss: 20579420.0\n",
      "Loss: 20635650.0\n",
      "Loss: 20587384.0\n",
      "Loss: 20527680.0\n",
      "Loss: 20512008.0\n",
      "Loss: 20481460.0\n",
      "Loss: 20471530.0\n",
      "Testing model\n",
      "Done. Test loss: 411859.15625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isidre/Documents/UAB/TFM/FootballAnalyticsTFM/.venv1/lib/python3.8/site-packages/spektral/layers/convolutional/conv.py:93: UserWarning: The adjacency matrix of dtype <dtype: 'int64'> is incompatible with the dtype of the node features <dtype: 'float32'> and has been automatically cast to <dtype: 'float32'>.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example shows how to perform regression of molecular properties with the\n",
    "QM9 database, using a simple GNN in disjoint mode.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data import DisjointLoader\n",
    "from spektral.datasets import QM9\n",
    "from spektral.layers import ECCConv, GlobalSumPool\n",
    "\n",
    "################################################################################\n",
    "# Config\n",
    "################################################################################\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "epochs = 100  # Number of training epochs\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "################################################################################\n",
    "# Load data\n",
    "################################################################################\n",
    "dataset = QM9(amount=1000)  # Set amount=None to train on whole dataset\n",
    "\n",
    "# Parameters\n",
    "F = dataset.n_node_features  # Dimension of node features\n",
    "S = dataset.n_edge_features  # Dimension of edge features\n",
    "n_out = dataset.n_labels  # Dimension of the target\n",
    "\n",
    "# Train/test split\n",
    "idxs = np.random.permutation(len(dataset))\n",
    "split = int(0.9 * len(dataset))\n",
    "idx_tr, idx_te = np.split(idxs, [split])\n",
    "dataset_tr, dataset_te = dataset[idx_tr], dataset[idx_te]\n",
    "\n",
    "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
    "\n",
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "class Net(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ECCConv(32, activation=\"relu\")\n",
    "        self.conv2 = ECCConv(32, activation=\"relu\")\n",
    "        self.global_pool = GlobalSumPool()\n",
    "        self.dense = Dense(n_out)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, e, i = inputs\n",
    "        x = self.conv1([x, a, e])\n",
    "        x = self.conv2([x, a, e])\n",
    "        output = self.global_pool([x, i])\n",
    "        output = self.dense(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Net()\n",
    "optimizer = Adam(learning_rate)\n",
    "loss_fn = MeanSquaredError()\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Fit model\n",
    "################################################################################\n",
    "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "step = loss = 0\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss += train_step(*batch)\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
    "        loss = 0\n",
    "\n",
    "################################################################################\n",
    "# Evaluate model\n",
    "################################################################################\n",
    "print(\"Testing model\")\n",
    "loss = 0\n",
    "for batch in loader_te:\n",
    "    inputs, target = batch\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss += loss_fn(target, predictions)\n",
    "loss /= loader_te.steps_per_epoch\n",
    "print(\"Done. Test loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deae8ace-922e-42cd-b2e9-572e5da96608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.57711800e+02  1.57709970e+02  1.57706990e+02  0.00000000e+00\n",
      "  1.32100000e+01 -3.87700000e-01  1.17100000e-01  5.04800000e-01\n",
      "  3.53641000e+01  4.47490000e-02 -4.04789300e+01 -4.04760620e+01\n",
      " -4.04751170e+01 -4.04985970e+01  6.46900000e+00 -3.95999595e+02\n",
      " -3.98643290e+02 -4.01014647e+02 -3.72471772e+02]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba18f2-ea47-4d23-9d34-893ce1aaf476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
