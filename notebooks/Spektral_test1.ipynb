{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "781a83ba-cd38-4957-89a9-037683ae2554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 19:32:46.271704: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-25 19:32:46.277214: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-25 19:32:47.183121: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 1 - Loss: 0.870 - Acc: 0.594 - Val loss: 0.583 - Val acc: 0.740\n",
      "New best val_loss 0.583\n",
      "Ep. 2 - Loss: 0.530 - Acc: 0.786 - Val loss: 0.404 - Val acc: 0.800\n",
      "New best val_loss 0.404\n",
      "Ep. 3 - Loss: 0.361 - Acc: 0.855 - Val loss: 0.330 - Val acc: 0.850\n",
      "New best val_loss 0.330\n",
      "Ep. 4 - Loss: 0.292 - Acc: 0.880 - Val loss: 0.251 - Val acc: 0.900\n",
      "New best val_loss 0.251\n",
      "Ep. 5 - Loss: 0.303 - Acc: 0.874 - Val loss: 0.295 - Val acc: 0.860\n",
      "Ep. 6 - Loss: 0.256 - Acc: 0.896 - Val loss: 0.227 - Val acc: 0.900\n",
      "New best val_loss 0.227\n",
      "Ep. 7 - Loss: 0.264 - Acc: 0.886 - Val loss: 0.212 - Val acc: 0.900\n",
      "New best val_loss 0.212\n",
      "Ep. 8 - Loss: 0.223 - Acc: 0.924 - Val loss: 0.212 - Val acc: 0.910\n",
      "New best val_loss 0.212\n",
      "Ep. 9 - Loss: 0.219 - Acc: 0.914 - Val loss: 0.221 - Val acc: 0.900\n",
      "Ep. 10 - Loss: 0.244 - Acc: 0.892 - Val loss: 0.174 - Val acc: 0.960\n",
      "New best val_loss 0.174\n",
      "Ep. 11 - Loss: 0.213 - Acc: 0.910 - Val loss: 0.300 - Val acc: 0.880\n",
      "Ep. 12 - Loss: 0.207 - Acc: 0.910 - Val loss: 0.227 - Val acc: 0.880\n",
      "Ep. 13 - Loss: 0.247 - Acc: 0.896 - Val loss: 0.153 - Val acc: 0.960\n",
      "New best val_loss 0.153\n",
      "Ep. 14 - Loss: 0.200 - Acc: 0.918 - Val loss: 0.214 - Val acc: 0.900\n",
      "Ep. 15 - Loss: 0.197 - Acc: 0.926 - Val loss: 0.252 - Val acc: 0.870\n",
      "Ep. 16 - Loss: 0.208 - Acc: 0.911 - Val loss: 0.214 - Val acc: 0.920\n",
      "Ep. 17 - Loss: 0.260 - Acc: 0.894 - Val loss: 0.207 - Val acc: 0.890\n",
      "Ep. 18 - Loss: 0.177 - Acc: 0.933 - Val loss: 0.146 - Val acc: 0.970\n",
      "New best val_loss 0.146\n",
      "Ep. 19 - Loss: 0.177 - Acc: 0.935 - Val loss: 0.134 - Val acc: 0.960\n",
      "New best val_loss 0.134\n",
      "Ep. 20 - Loss: 0.153 - Acc: 0.946 - Val loss: 0.137 - Val acc: 0.960\n",
      "Ep. 21 - Loss: 0.158 - Acc: 0.938 - Val loss: 0.127 - Val acc: 0.950\n",
      "New best val_loss 0.127\n",
      "Ep. 22 - Loss: 0.242 - Acc: 0.894 - Val loss: 0.143 - Val acc: 0.950\n",
      "Ep. 23 - Loss: 0.187 - Acc: 0.920 - Val loss: 0.131 - Val acc: 0.960\n",
      "Ep. 24 - Loss: 0.191 - Acc: 0.923 - Val loss: 0.237 - Val acc: 0.870\n",
      "Ep. 25 - Loss: 0.164 - Acc: 0.935 - Val loss: 0.159 - Val acc: 0.950\n",
      "Ep. 26 - Loss: 0.137 - Acc: 0.947 - Val loss: 0.136 - Val acc: 0.950\n",
      "Ep. 27 - Loss: 0.220 - Acc: 0.904 - Val loss: 0.142 - Val acc: 0.950\n",
      "Ep. 28 - Loss: 0.201 - Acc: 0.926 - Val loss: 0.135 - Val acc: 0.970\n",
      "Ep. 29 - Loss: 0.171 - Acc: 0.933 - Val loss: 0.130 - Val acc: 0.950\n",
      "Ep. 30 - Loss: 0.141 - Acc: 0.946 - Val loss: 0.230 - Val acc: 0.900\n",
      "Ep. 31 - Loss: 0.143 - Acc: 0.941 - Val loss: 0.184 - Val acc: 0.920\n",
      "Early stopping (best val_loss: 0.12695135500282048)\n",
      "Done. Test loss: 0.1428. Test acc: 0.93\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example shows how to define your own dataset and use it to train a\n",
    "non-trivial GNN with message-passing and pooling layers.\n",
    "The script also shows how to implement fast training and evaluation functions\n",
    "in disjoint mode, with early stopping and accuracy monitoring.\n",
    "The dataset that we create is a simple synthetic task in which we have random\n",
    "graphs with randomly-colored nodes. The goal is to classify each graph with the\n",
    "color that occurs the most on its nodes. For example, given a graph with 2\n",
    "colors and 3 nodes:\n",
    "x = [[1, 0],\n",
    "     [1, 0],\n",
    "     [0, 1]],\n",
    "the corresponding target will be [1, 0].\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GCSConv, GlobalAvgPool\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "\n",
    "################################################################################\n",
    "# Config\n",
    "################################################################################\n",
    "learning_rate = 1e-2  # Learning rate\n",
    "epochs = 400  # Number of training epochs\n",
    "es_patience = 10  # Patience for early stopping\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Load data\n",
    "################################################################################\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset of random colored graphs.\n",
    "    The task is to classify each graph with the color which occurs the most in\n",
    "    its nodes.\n",
    "    The graphs have `n_colors` colors, of at least `n_min` and at most `n_max`\n",
    "    nodes connected with probability `p`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_samples, n_colors=3, n_min=10, n_max=100, p=0.1, **kwargs):\n",
    "        self.n_samples = n_samples\n",
    "        self.n_colors = n_colors\n",
    "        self.n_min = n_min\n",
    "        self.n_max = n_max\n",
    "        self.p = p\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        def make_graph():\n",
    "            n = np.random.randint(self.n_min, self.n_max)\n",
    "            colors = np.random.randint(0, self.n_colors, size=n)\n",
    "\n",
    "            # Node features\n",
    "            x = np.zeros((n, self.n_colors))\n",
    "            x[np.arange(n), colors] = 1\n",
    "\n",
    "            # Edges\n",
    "            a = np.random.rand(n, n) <= self.p\n",
    "            a = np.maximum(a, a.T).astype(int)\n",
    "            a = sp.csr_matrix(a)\n",
    "\n",
    "            # Labels\n",
    "            y = np.zeros((self.n_colors,))\n",
    "            color_counts = x.sum(0)\n",
    "            y[np.argmax(color_counts)] = 1\n",
    "\n",
    "            return Graph(x=x, a=a, y=y)\n",
    "\n",
    "        # We must return a list of Graph objects\n",
    "        return [make_graph() for _ in range(self.n_samples)]\n",
    "\n",
    "\n",
    "data = MyDataset(1000, transforms=NormalizeAdj())\n",
    "\n",
    "# Train/valid/test split\n",
    "idxs = np.random.permutation(len(data))\n",
    "split_va, split_te = int(0.8 * len(data)), int(0.9 * len(data))\n",
    "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\n",
    "data_tr = data[idx_tr]\n",
    "data_va = data[idx_va]\n",
    "data_te = data[idx_te]\n",
    "\n",
    "# Data loaders\n",
    "loader_tr = DisjointLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
    "loader_va = DisjointLoader(data_va, batch_size=batch_size)\n",
    "loader_te = DisjointLoader(data_te, batch_size=batch_size)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "class Net(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCSConv(32, activation=\"relu\")\n",
    "        self.conv2 = GCSConv(32, activation=\"relu\")\n",
    "        self.conv3 = GCSConv(32, activation=\"relu\")\n",
    "        self.global_pool = GlobalAvgPool()\n",
    "        self.dense = Dense(data.n_labels, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        x = self.conv2([x, a])\n",
    "        x = self.conv3([x, a])\n",
    "        output = self.global_pool([x, i])\n",
    "        output = self.dense(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Net()\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Fit model\n",
    "################################################################################\n",
    "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(categorical_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
    "\n",
    "\n",
    "epoch = step = 0\n",
    "best_val_loss = np.inf\n",
    "best_weights = None\n",
    "patience = es_patience\n",
    "results = []\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss, acc = train_step(*batch)\n",
    "    results.append((loss, acc))\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss, val_acc = evaluate(loader_va)\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), val_loss, val_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Check if loss improved for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = es_patience\n",
    "            print(\"New best val_loss {:.3f}\".format(val_loss))\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping (best val_loss: {})\".format(best_val_loss))\n",
    "                break\n",
    "        results = []\n",
    "\n",
    "################################################################################\n",
    "# Evaluate model\n",
    "################################################################################\n",
    "model.set_weights(best_weights)  # Load best model\n",
    "test_loss, test_acc = evaluate(loader_te)\n",
    "print(\"Done. Test loss: {:.4f}. Test acc: {:.2f}\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9939c53-8d92-4da8-b9ee-3e6b7c427373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62c260-0b11-4b69-85d4-fae4780799b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac063e50-440a-405a-b182-ae13ae6fbf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a389bb1-1cce-4deb-9476-906d4d143034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv1': venv)",
   "language": "python",
   "name": "python3810jvsc74a57bd06e0aed3ed461d30436457c9d897a6620dc7a130baf50f7322b8852eb4291a86a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
